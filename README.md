# ğŸ¬ BERT IMDB Sentiment Analysis

This project fine-tunes **BERT (bert-base-uncased)** on the [IMDB 50K Movie Reviews Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) to classify movie reviews as **positive** or **negative**.

---

## ğŸ“Œ Features
- Dataset: [IMDB 50K Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).
- Fine-tunes **BERT-base-uncased** for binary classification.
- Evaluation metrics: Accuracy, Precision, Recall, F1 Score.
- Training with Hugging Face **Trainer API** + Early Stopping.
- Saves fine-tuned model and tokenizer.
- Easy inference using a pipeline.

---

## ğŸ“‚ Project Structure


bert-imdb-sentiment/
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ bert_imdb_sentiment.ipynb # Original Colab notebook
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ dataset.py # IMDbDataset class
â”‚ â”œâ”€â”€ train.py # Training script
â”‚ â”œâ”€â”€ evaluate.py # Evaluation & inference
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ README.md # Project description
â””â”€â”€ .gitignore # Ignore unnecessary files



---

## âš™ï¸ Installation
Clone the repository and install dependencies:
```bash
git clone https://github.com/USERNAME/bert-imdb-sentiment.git
cd bert-imdb-sentiment
pip install -r requirements.txt


ğŸš€ Training
Run the training script:
python src/train.py


The best model will be saved in:
results/model/

ğŸ” Evaluation
Evaluate the model or run inference:

python src/evaluate.py

ğŸ’¡ Inference Example

from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline
import torch

model = BertForSequenceClassification.from_pretrained("results/model")
tokenizer = BertTokenizer.from_pretrained("results/model")

pipeline = TextClassificationPipeline(
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

print(pipeline("This movie was amazing and full of emotions!"))
ğŸ“Œ Expected output:

[{'label': 'POSITIVE', 'score': 0.98}]

ğŸ“ License
This project is open-source for academic and experimental use





